
## 🎓 نمونه سوال امتحانی – شبکه‌های عصبی مصنوعی پارت 3

### 🧠 سوال ۱:

پرسپترون چیست؟ چه محدودیتی دارد؟

**✅ پاسخ:**
پرسپترون یک مدل ساده از نورون مصنوعی است که فقط قادر به حل مسائل **خطی جداپذیر** است. یعنی اگر داده‌ها با یک خط یا صفحه قابل جدا شدن نباشند، **قانون یادگیری پرسپترون همگرا نمی‌شود** و نمی‌تواند آن‌ها را به‌درستی طبقه‌بندی کند.
⚠️ **محدودیت اصلی:** عدم توانایی در یادگیری مسائل غیرخطی.

---

### ⚙️ سوال ۲:

فرمول به‌روزرسانی وزن‌ها در قانون دلتا چیست؟ توضیح دهید.

**✅ پاسخ:**
فرمول کلی:

$$
\Delta w_i = \eta (t - o)x_i
$$

* $\eta$: نرخ یادگیری (Learning Rate)
* $t$: خروجی هدف
* $o$: خروجی واقعی
* $x_i$: ورودی به نرون

در این قانون، از **گرادیان نزولی** برای کاهش خطا استفاده می‌شود. وزن‌ها در جهت خلاف شیب خطا حرکت می‌کنند تا به مینیمم محلی برسند.
📉 **هدف:** کاهش تابع خطا.

---

### 🧮 سوال ۳:

تفاوت گرادیان نزولی دسته‌ای (Batch) و تصادفی (SGD) در چیست؟

**✅ پاسخ:**

| روش          | ویژگی                          | مزیت                         | عیب           |
| ------------ | ------------------------------ | ---------------------------- | ------------- |
| 🎲 **SGD**   | استفاده از یک داده در هر مرحله | سریع‌تر، عبور از مینیمم محلی | نوسان بالا    |
| 📦 **Batch** | استفاده از کل داده‌ها          | به‌روزرسانی دقیق             | کند و پرهزینه |

---

### 🧪 سوال ۴:

فرایند کلی آموزش شبکه عصبی چند لایه (MLP) را به‌اختصار نام ببرید.

**✅ پاسخ:**

1. شناسایی ورودی و خروجی‌ها 📥📤
2. نرمال‌سازی داده‌ها 📏
3. طراحی ساختار شبکه 🧱
4. آموزش با داده‌های آموزشی 📚
5. اعتبارسنجی با داده‌های جدید ✅
6. ارزیابی نهایی و استفاده برای پیش‌بینی 🔍


---

### 🔄 سوال ۵:

الگوریتم **پس‌انتشار خطا (Back Propagation)** چگونه عمل می‌کند؟

**✅ پاسخ:**
الگوریتم BP برای آموزش شبکه‌های چند لایه استفاده می‌شود. این الگوریتم به صورت زیر عمل می‌کند:

1. **جلو رفتن سیگنال (Forward Pass):** داده‌ها از ورودی به خروجی منتقل می‌شوند.
2. **محاسبه خطا (Error):** اختلاف خروجی شبکه با مقدار هدف محاسبه می‌شود.
3. **انتشار به عقب (Backward Pass):** خطا به صورت معکوس در شبکه پخش می‌شود و وزن‌ها بر اساس گرادیان نزولی به‌روزرسانی می‌شوند.

📌 از مشتق تابع فعال‌سازی (مانند سیگموید) استفاده می‌شود.
🎯 هدف: کاهش خطا در هر مرحله تکرار (epoch).

---

### 🔢 سوال ۶:

فرمول محاسبه **گرادیان خطا در لایه خروجی و لایه پنهان** را بنویسید.

**✅ پاسخ:**

* برای نورون خروجی:

$$
\delta_k = o_k (1 - o_k)(t_k - o_k)
$$

* برای نورون پنهان:

$$
\delta_j = o_j (1 - o_j) \sum_k W_{kj} \delta_k
$$

❗این فرمول‌ها نشان‌دهنده تاثیر خطای لایه بعد روی وزن‌های فعلی هستند.
📉 با استفاده از این δها، وزن‌ها به صورت زیر تغییر می‌کنند:

$$
W = W + \eta \cdot \delta \cdot x
$$

---

### 🔁 سوال ۷:

قانون **ممنتوم (Momentum)** در به‌روزرسانی وزن‌ها چیست و چه مزیتی دارد؟

**✅ پاسخ:**

برای به‌روزرسانی وزن‌ها با در نظر گرفتن شیب مرحله قبل:

$$
\Delta W_{ji}(n) = \eta \cdot \delta_j \cdot x_{ji} + \alpha \cdot \Delta W_{ji}(n-1)
$$

که در آن:

* $\eta$: نرخ یادگیری (Learning Rate)
* $\alpha$: ضریب ممنتوم (که معمولاً بین ۰ و ۱ انتخاب می‌شود)
* $\delta_j$: خطای نرون
* $x_{ji}$: ورودی به نرون
* $\Delta W_{ji}(n-1)$: تغییر وزن در تکرار قبلی

📌 **توضیح دقیق همان‌طور که در فایل آمده:**

> مقدار $\alpha$ در بازه ۰ تا ۱ قرار دارد و اضافه کردن ممنتوم باعث می‌شود حرکت در مسیر قبلی حفظ شود و:
>
> * از گیر افتادن در **مینیمم محلی** جلوگیری شود ✅
> * از گیر افتادن در سطوح صاف تابع خطا جلوگیری شود ⚠️
> * با افزایش تدریجی مقدار گام، **سرعت جستجو افزایش یابد** 🚀



---

### 🚫 سوال ۸:

سه شرط برای **توقف الگوریتم پس‌انتشار خطا (BP)** چیست؟

**✅ پاسخ:**

1. رسیدن به تعداد مشخصی از تکرارها (epochs) 🕒
2. کاهش خطا به زیر یک مقدار معین ✅
3. افزایش خطا در داده‌های اعتبارسنجی (برای جلوگیری از overfitting) 📉

---

### 🧩 سوال ۹:

تفاوت بین سه نوع **گرادیان نزولی (Gradient Descent)** را بنویسید.

**✅ پاسخ:**

| نوع                 | ویژگی                    | مزیت             | محدودیت                     |
| ------------------- | ------------------------ | ---------------- | --------------------------- |
| 🧺 Batch            | کل داده‌ها در هر تکرار   | دقیق و پایدار    | کند و نیازمند حافظه زیاد    |
| 🎲 Stochastic (SGD) | یک داده در هر تکرار      | سریع و سبک       | نوسان بالا                  |
| 🧬 Mini-batch       | گروه‌های کوچک از داده‌ها | تعادل سرعت و دقت | نیاز به انتخاب اندازه مناسب |

---

