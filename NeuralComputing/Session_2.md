### **بخش اول: تستی (چهارگزینه‌ای) با ایموجی**

**سؤال 1: اولین مدل ریاضی نرون مصنوعی توسط چه کسانی ارائه شد؟**  
A) جان هاپفیلد  
B) کالمن و مک‌کورد  
C) پیتس و مک‌کلاخ  
D) سایمون و مینسکی  
**پاسخ صحیح: C**  
**توضیح:** مدل ریاضی نرون توسط پیتس و مک‌کلاخ در سال 1943 معرفی شد.  
**ایموجی:** 🧠✨

---

**سؤال 2: وظیفه اصلی دندریت‌ها در نرون چیست؟**  
A) ارسال سیگنال  
B) پردازش اطلاعات  
C) دریافت سیگنال  
D) تنظیم وزن‌ها  
**پاسخ صحیح: C**  
**توضیح:** دندریت‌ها بخش ورودی نرون هستند که سیگنال‌ها را دریافت می‌کنند.  
**ایموجی:** ⚡️➡️🧠

---

**سؤال 3: تابع فعال‌سازی ReLU چگونه تعریف می‌شود؟**  
A) $f(x) = \tanh(x)$  
B) $f(x) = \frac{1}{1 + e^{-x}}$  
C) $f(x) = \max(0, x)$  
D) $f(x) = x^2$  
**پاسخ صحیح: C**  
**ایموجی:** 🔺⬅️📈

---

**سؤال 4: کدام نوع شبکه عصبی برای حل مشکل XOR معرفی شد؟**  
A) پرسپترون تک‌لایه  
B) شبکه بازخوردی  
C) شبکه چندلایه (MLP)  
D) RBF  
**پاسخ صحیح: C**  
**توضیح:** مشکل XOR توسط شبکه‌های چندلایه قابل حل است.  
**ایموجی:** ❌➡️✅✨

---

**سؤال 5: در مدل پرسپترون، خروجی چه زمانی 1 خواهد بود؟**  
A) وقتی مجموع ورودی‌ها منفی باشد  
B) وقتی وزن‌ها برابر صفر باشند  
C) وقتی net ≥ 0 باشد  
D) وقتی net < 0 باشد  
**پاسخ صحیح: C**  
**ایموجی:** ➕➕ = ✅

---

**سؤال 6: کدام مورد جزو انواع شبکه‌های عصبی نیست؟**  
A) Hopfield  
B) Feedforward  
C) Kohonen  
D) Hadoop  
**پاسخ صحیح: D**  
**توضیح:** Hadoop یک چارچوب برای مدیریت داده است، نه شبکه عصبی.  
**ایموجی:** 🧠❌💾

---

**سؤال 7: در شبکه‌های عصبی، "Overfitting" به چه معناست؟**  
A) آموزش کم  
B) افزایش تعداد نرون‌ها  
C) یادگیری بیش از حد روی داده‌های آموزش  
D) عملکرد بالا در داده‌های جدید  
**پاسخ صحیح: C**  
**ایموجی:** 📚📚📚❌

---

**سؤال 8: چه بخشی از نرون وظیفه انتقال سیگنال به نرون‌های دیگر را دارد؟**  
A) دندریت  
B) آکسون  
C) سیناپس  
D) سوما  
**پاسخ صحیح: B**  
**توضیح:** آکسون سیگنال را از بدنه سلول به نرون‌های دیگر منتقل می‌کند.  
**ایموجی:** 🧠➡️📤

---

**سؤال 9: در شبکه‌های عصبی، چه عاملی بین نرون‌ها ارتباط برقرار می‌کند؟**  
A) لایه پنهان  
B) تابع فعال‌سازی  
C) سیناپس  
D) گرادیان نزولی  
**پاسخ صحیح: C**  
**توضیح:** سیناپس‌ها پل ارتباطی نرون‌ها هستند.  
**ایموجی:** 🔗🧠🔗

---

**سؤال 10: کدامیک از گزینه‌ها نوعی تابع فعال‌سازی نیست؟**  
A) ReLU  
B) Tanh  
C) Logistic  
D) Mean Square  
**پاسخ صحیح: D**  
**توضیح:** میانگین مربعات خطا یک تابع خطاست، نه تابع فعال‌سازی.  
**ایموجی:** ⚠️➖❌

---

### **بخش دوم: تشریحی**

**سؤال 1: نقش لایه پنهان در یک شبکه عصبی چیست؟**  
**پاسخ:** لایه پنهان باعث ایجاد توانایی برای یادگیری روابط **غیرخطی** بین ورودی‌ها و خروجی‌ها در شبکه می‌شود. این لایه اطلاعات را از ورودی گرفته و از طریق وزن‌ها و تابع فعال‌سازی، ویژگی‌های مهم را استخراج می‌کند.  
**ایموجی:** ⚙️🔍➡️📊

---

**سؤال 2: تفاوت بین توابع فعال‌سازی سیگموئید و تانژانت هیپربولیک چیست؟**  
**پاسخ:** تابع **سیگموئید** خروجی بین 0 و 1 دارد، در حالی که تابع **تانژانت هیپربولیک (tanh)** خروجی بین -1 و 1 دارد. این تفاوت باعث می‌شود که tanh در بسیاری از موارد سریع‌تر به تعادل برسد.  
**ایموجی:** 0️⃣↔️1️⃣ و ➖1️⃣↔️➕1️⃣

---

**سؤال 3: الگوریتم گرادیان نزولی چگونه عمل می‌کند؟**  
**پاسخ:** این الگوریتم برای کاهش مقدار خطا در شبکه عصبی، با حرکت در جهت **منفی گرادیان تابع خطا**، وزن‌ها را به گونه‌ای تنظیم می‌کند که خطا کاهش یابد.  
**ایموجی:** ⬇️📉🔧

---

### **بخش سوم: تشریحی**

**سؤال 4: مراحل آموزش یک شبکه عصبی را نام ببرید.**  
**پاسخ:**  
1. **آموزش (Training):** یادگیری الگوهای داده‌های ورودی  
2. **تعمیم (Generalization):** توانایی پاسخ به ورودی‌های جدید  
3. **اجرا (Execution):** استفاده از شبکه برای پیش‌بینی یا طبقه‌بندی  
**ایموجی:** ✍️➡️🧠➡️🔮

---

**سؤال 5: شرط پایان الگوریتم BP چیست؟**  
**پاسخ:**  
* رسیدن به تعداد مشخصی از تکرار  
* کاهش خطا به کمتر از یک مقدار معین  
* افزایش خطا در داده‌های اعتبارسنجی (برای جلوگیری از overfitting)  
**ایموجی:** ⏳✅❌

---

**سؤال 6: مشکلات روش Gradient Descent را نام ببرید.**  
**پاسخ:**  
* ممکن است زمان زیادی برای همگرایی لازم باشد  
* امکان گیر افتادن در مینیمم محلی وجود دارد  
**ایموجی:** 🐌⚠️⛰️

---

### **بخش چهارم: تستی (چهارگزینه‌ای)**

**سؤال 8: چرا استفاده از تابع فعال‌سازی ReLU در شبکه‌های عصبی محبوب است؟**  
**پاسخ:** تابع ReLU با فرمول $f(x) = \max(0, x)$ تعریف می‌شود. این تابع بسیار ساده، سریع، و موثر است ⚡. مزیت اصلی آن نسبت به توابعی مثل سیگموئید یا تانژانت هیپربولیک این است که باعث **انتشار بهتر گرادیان** در شبکه می‌شود و مشکل **vanishing gradient** را کاهش می‌دهد. در واقع، با ReLU، نرون‌هایی که ورودی مثبت دارند فعال می‌شوند (روشن می‌شن) 💡 و نرون‌هایی که ورودی منفی دارند خاموش می‌مونن ❌.

---

**سؤال 9: چرا مشکل Overfitting در شبکه‌های عصبی رخ می‌دهد؟ چگونه می‌توان از آن جلوگیری کرد؟**  
**پاسخ:** وقتی شبکه بیش از حد روی داده‌های آموزشی تمرین می‌کند 🧠📚، ممکنه ویژگی‌های خاص و حتی نویز موجود در آن داده‌ها را یاد بگیره، اما نتونه برای داده‌های جدید عملکرد خوبی داشته باشه ❗ این یعنی **توانایی تعمیم کاهش پیدا می‌کنه**. برای جلوگیری از Overfitting می‌توان از روش‌هایی مثل **اعتبارسنجی** (validation) ✅، **Dropout** ❎، یا **توقف زودهنگام** ⏹️ استفاده کرد.

---

**سؤال 10: در یک نرون مصنوعی، چه نقشی برای سیناپس‌ها در نظر گرفته شده است؟**  
**پاسخ:** سیناپس‌ها مانند پل ارتباطی بین نرون‌ها عمل می‌کنند 🔗. هر سیناپس دارای یک وزن است ⚖️ که نشان‌دهنده شدت ارتباط بین نرون‌هاست. این وزن‌ها در حین آموزش تنظیم می‌شن تا شبکه بتونه به پاسخ صحیح برسه ✅. در واقع، سیناپس‌ها هستند که مسیر انتقال اطلاعات را مشخص می‌کنند و نقش حیاتی در فرآیند یادگیری دارند.

---

**سؤال 11: ساختار یک نرون مصنوعی از چه اجزایی تشکیل شده است؟**  
**پاسخ:** یک نرون مصنوعی شبیه‌سازی ساده‌ای از نرون زیستی 🧠 هست که از سه بخش اصلی تشکیل شده:  
1. **دندریت‌ها (Dendrites):** دریافت‌کننده‌ی سیگنال‌های ورودی 📥  
2. **بدنه سلول (Soma):** پردازشگر اطلاعات و محاسبه‌کننده‌ی مجموع وزن‌دار سیگنال‌ها ⚙️  
3. **آکسون (Axon):** انتقال‌دهنده سیگنال خروجی به نرون‌های دیگر 📤  
   همه‌ی این اجزا با هم کار می‌کنن تا یک تصمیم‌گیری ساده یا پیچیده در شبکه انجام بشه 💡.

---

**سؤال 12: شبکه‌های چندلایه (MLP) چگونه کار می‌کنند؟**  
**پاسخ:** در شبکه‌های چندلایه یا **MLP (Multi-Layer Perceptron)**، اطلاعات به صورت مرحله‌ای بین لایه‌ها منتقل می‌شن 🔄. لایه‌ها شامل:  
* **لایه ورودی**: دریافت‌کننده داده‌ها 🎯  
* **لایه پنهان (Hidden layer)**: استخراج‌کننده ویژگی‌های پیچیده 🌐  
* **لایه خروجی**: تولیدکننده پاسخ نهایی 📊  
  این شبکه با استفاده از توابع فعال‌سازی غیرخطی مثل ReLU یا Sigmoid قادره روابط پیچیده رو یاد بگیره و مسائلی مثل **XOR** رو حل کنه 🧩✨.

---

**سؤال 13: چه تفاوتی بین توابع فعال‌سازی Sigmoid و ReLU وجود دارد؟**  
**پاسخ:** تابع **Sigmoid** خروجی رو بین 0 تا 1 نگه می‌داره و مناسب برای مدل‌هایی با احتمال یا دسته‌بندی دوگانه هست 🎯. اما ممکنه در شبکه‌های عمیق باعث مشکل **vanishing gradient** بشه 😵. در مقابل، تابع **ReLU**، خروجی رو به صورت مستقیم و غیرمنفی بازمی‌گردونه و ساده‌تر و سریع‌تره ⚡. ReLU باعث میشه که فقط نرون‌های فعال در فرآیند یادگیری درگیر باشن 🔛 و سرعت آموزش افزایش پیدا کنه 🚀.

---

**سؤال 14: چرا وجود لایه‌های پنهان برای حل مسائل پیچیده مثل XOR ضروری است؟**  
**پاسخ:** چون XOR یک مسئله **غیر خطی** است و شبکه‌های تک‌لایه نمی‌توانند آن را یاد بگیرند. لایه پنهان باعث افزایش قدرت تفکیک‌پذیری شبکه می‌شود.  
**ایموجی:** ❌➕❌ = ✅ با کمک 🧠🧩

---
