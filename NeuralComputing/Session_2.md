### **بخش اول: تشریحی**

**سؤال 1: نقش لایه پنهان در یک شبکه عصبی چیست؟**  
**پاسخ:**

لایه پنهان باعث ایجاد توانایی برای یادگیری روابط **غیرخطی** بین ورودی‌ها و خروجی‌ها در شبکه می‌شود. این لایه اطلاعات را از ورودی گرفته و از طریق وزن‌ها و تابع فعال‌سازی، ویژگی‌های مهم را استخراج می‌کند. ⚙️🔍➡️📊

---

**سؤال 2: تفاوت بین توابع فعال‌سازی سیگموئید و تانژانت هیپربولیک چیست؟**  
**پاسخ:**

تابع **سیگموئید** خروجی را بین 0 تا 1 نگه می‌دارد، در حالی که تابع **تانژانت هیپربولیک (tanh)** خروجی را بین -1 و 1 دارد. این تفاوت باعث می‌شود که tanh در بسیاری از موارد سریع‌تر به تعادل برسد. 0️⃣↔️1️⃣ و ➖1️⃣↔️➕1️⃣

---

**سؤال 3: الگوریتم گرادیان نزولی چگونه عمل می‌کند؟**  
**پاسخ:**

این الگوریتم برای کاهش مقدار خطا در شبکه عصبی، با حرکت در جهت **منفی گرادیان تابع خطا**، وزن‌ها را به گونه‌ای تنظیم می‌کند که خطا کاهش یابد. ⬇️📉🔧

---

**سؤال 4: مراحل آموزش یک شبکه عصبی را نام ببرید.**  
**پاسخ:** 

1. **آموزش (Training):** یادگیری الگوهای داده‌های ورودی ✍️  
2. **تعمیم (Generalization):** توانایی پاسخ به ورودی‌های جدید 🧠  
3. **اجرا (Execution):** استفاده از شبکه برای پیش‌بینی یا طبقه‌بندی 🔮

---

**سؤال 5: شرط پایان الگوریتم BP چیست؟**  
**پاسخ:** 

* رسیدن به تعداد مشخصی از تکرار ⏳  
* کاهش خطا به کمتر از یک مقدار معین ✅  
* افزایش خطا در داده‌های اعتبارسنجی (برای جلوگیری از overfitting) ❌

---

**سؤال 6: مشکلات روش Gradient Descent را نام ببرید.**  
**پاسخ:**

* ممکن است زمان زیادی برای همگرایی لازم باشد 🐌  
* امکان گیر افتادن در مینیمم محلی وجود دارد ⚠️

---

**سؤال 7: چرا مشکل Overfitting در شبکه‌های عصبی رخ می‌دهد؟ چگونه می‌توان از آن جلوگیری کرد؟**  
**پاسخ:**

وقتی شبکه بیش از حد روی داده‌های آموزشی تمرین می‌کند 🧠📚، ممکن است ویژگی‌های خاص و حتی نویز موجود در آن داده‌ها را یاد بگیرد، اما نتواند برای داده‌های جدید عملکرد خوبی داشته باشد ❗. برای جلوگیری از Overfitting می‌توان از روش‌هایی مثل **اعتبارسنجی** (validation) ✅، **Dropout** ❎، یا **توقف زودهنگام** ⏹️ استفاده کرد.

---

**سؤال 8: در یک نرون مصنوعی، چه نقشی برای سیناپس‌ها در نظر گرفته شده است؟**  
**پاسخ:**

سیناپس‌ها مانند پل ارتباطی بین نرون‌ها عمل می‌کنند 🔗. هر سیناپس دارای یک وزن است ⚖️ که نشان‌دهنده شدت ارتباط بین نرون‌هاست. این وزن‌ها در حین آموزش تنظیم می‌شوند تا شبکه بتواند به پاسخ صحیح برسد ✅.

---

**سؤال 9: ساختار یک نرون مصنوعی از چه اجزایی تشکیل شده است؟**  
**پاسخ:**

یک نرون مصنوعی شبیه‌سازی ساده‌ای از نرون زیستی 🧠 است که از سه بخش اصلی تشکیل شده:  
1. **دندریت‌ها (Dendrites):** دریافت‌کننده‌ی سیگنال‌های ورودی 📥  
2. **بدنه سلول (Soma):** پردازشگر اطلاعات و محاسبه‌کننده‌ی مجموع وزن‌دار سیگنال‌ها ⚙️  
3. **آکسون (Axon):** انتقال‌دهنده سیگنال خروجی به نرون‌های دیگر 📤  

---

**سؤال 10: شبکه‌های چندلایه (MLP) چگونه کار می‌کنند؟**  
**پاسخ:**

در شبکه‌های چندلایه یا **MLP (Multi-Layer Perceptron)**، اطلاعات به صورت مرحله‌ای بین لایه‌ها منتقل می‌شوند 🔄. لایه‌ها شامل:  
* **لایه ورودی:** دریافت‌کننده داده‌ها 🎯  
* **لایه پنهان (Hidden layer):** استخراج‌کننده ویژگی‌های پیچیده 🌐  
* **لایه خروجی:** تولیدکننده پاسخ نهایی 📊  

---

**سؤال 11: چه تفاوتی بین توابع فعال‌سازی Sigmoid و ReLU وجود دارد؟**  
**پاسخ:**

تابع **Sigmoid** خروجی را بین 0 تا 1 نگه می‌دارد و مناسب برای مدل‌هایی با احتمال یا دسته‌بندی دوگانه است 🎯. اما ممکن است در شبکه‌های عمیق باعث مشکل **vanishing gradient** شود 😵. در مقابل، تابع **ReLU**، خروجی را به صورت مستقیم و غیرمنفی بازمی‌گرداند و ساده‌تر و سریع‌تر است ⚡.

---

**سؤال 12: چرا وجود لایه‌های پنهان برای حل مسائل پیچیده مثل XOR ضروری است؟**  
**پاسخ:**

چون XOR یک مسئله **غیر خطی** است و شبکه‌های تک‌لایه نمی‌توانند آن را یاد بگیرند. لایه پنهان باعث افزایش قدرت تفکیک‌پذیری شبکه می‌شود. ❌➕❌ = ✅

---


**سؤال 13: چرا استفاده از تابع فعال‌سازی ReLU در شبکه‌های عصبی محبوب است؟**  
**پاسخ:**

تابع ReLU با فرمول $f(x) = \max(0, x)$ تعریف می‌شود. این تابع بسیار ساده، سریع، و موثر است ⚡. مزیت اصلی آن نسبت به توابعی مثل سیگموئید یا تانژانت هیپربولیک این است که باعث **انتشار بهتر گرادیان** در شبکه می‌شود و مشکل **vanishing gradient** را کاهش می‌دهد. در واقع، با ReLU، نرون‌هایی که ورودی مثبت دارند فعال می‌شوند (روشن می‌شن) 💡 و نرون‌هایی که ورودی منفی دارند خاموش می‌مونن ❌.

---

**سؤال 14: چرا مشکل Overfitting در شبکه‌های عصبی رخ می‌دهد؟ چگونه می‌توان از آن جلوگیری کرد؟**  
**پاسخ:**

وقتی شبکه بیش از حد روی داده‌های آموزشی تمرین می‌کند 🧠📚، ممکنه ویژگی‌های خاص و حتی نویز موجود در آن داده‌ها را یاد بگیره، اما نتونه برای داده‌های جدید عملکرد خوبی داشته باشه ❗. این یعنی **توانایی تعمیم کاهش پیدا می‌کنه**. برای جلوگیری از Overfitting می‌توان از روش‌هایی مثل **اعتبارسنجی** (validation) ✅، **Dropout** ❎، یا **توقف زودهنگام** ⏹️ استفاده کرد.

---

**سؤال 15: در یک نرون مصنوعی، چه نقشی برای سیناپس‌ها در نظر گرفته شده است؟**  
**پاسخ:**

سیناپس‌ها مانند پل ارتباطی بین نرون‌ها عمل می‌کنند 🔗. هر سیناپس دارای یک وزن است ⚖️ که نشان‌دهنده شدت ارتباط بین نرون‌هاست. این وزن‌ها در حین آموزش تنظیم می‌شوند تا شبکه بتواند به پاسخ صحیح برسد ✅. در واقع، سیناپس‌ها هستند که مسیر انتقال اطلاعات را مشخص می‌کنند و نقش حیاتی در فرآیند یادگیری دارند.

---

**سؤال 16: ساختار یک نرون مصنوعی از چه اجزایی تشکیل شده است؟**  
**پاسخ:**

یک نرون مصنوعی شبیه‌سازی ساده‌ای از نرون زیستی 🧠 است که از سه بخش اصلی تشکیل شده:  
1. **دندریت‌ها (Dendrites):** دریافت‌کننده‌ی سیگنال‌های ورودی 📥  
2. **بدنه سلول (Soma):** پردازشگر اطلاعات و محاسبه‌کننده‌ی مجموع وزن‌دار سیگنال‌ها ⚙️  
3. **آکسون (Axon):** انتقال‌دهنده سیگنال خروجی به نرون‌های دیگر 📤  

---

**سؤال 17: شبکه‌های چندلایه (MLP) چگونه کار می‌کنند؟**  
**پاسخ:**

در شبکه‌های چندلایه یا **MLP (Multi-Layer Perceptron)**، اطلاعات به صورت مرحله‌ای بین لایه‌ها منتقل می‌شوند 🔄. لایه‌ها شامل:  
* **لایه ورودی:** دریافت‌کننده داده‌ها 🎯  
* **لایه پنهان (Hidden layer):** استخراج‌کننده ویژگی‌های پیچیده 🌐  
* **لایه خروجی:** تولیدکننده پاسخ نهایی 📊  

---

**سؤال 18: چه تفاوتی بین توابع فعال‌سازی Sigmoid و ReLU وجود دارد؟**  
**پاسخ:**

تابع **Sigmoid** خروجی را بین 0 تا 1 نگه می‌دارد و مناسب برای مدل‌هایی با احتمال یا دسته‌بندی دوگانه است 🎯. اما ممکن است در شبکه‌های عمیق باعث مشکل **vanishing gradient** شود 😵. در مقابل، تابع **ReLU**، خروجی را به صورت مستقیم و غیرمنفی بازمی‌گرداند و ساده‌تر و سریع‌تر است ⚡.

---




### **بخش سوم: تستی (چهارگزینه‌ای)**

**سؤال 1: اولین مدل ریاضی نرون مصنوعی توسط چه کسانی ارائه شد؟**  
A) جان هاپفیلد  
B) کالمن و مک‌کورد  
C) پیتس و مک‌کلاخ  
D) سایمون و مینسکی  
**پاسخ صحیح: C**  
**توضیح:** مدل ریاضی نرون توسط پیتس و مک‌کلاخ در سال 1943 معرفی شد. 🧠✨

---

**سؤال 2: وظیفه اصلی دندریت‌ها در نرون چیست؟**  
A) ارسال سیگنال  
B) پردازش اطلاعات  
C) دریافت سیگنال  
D) تنظیم وزن‌ها  
**پاسخ صحیح: C**  
**توضیح:** دندریت‌ها بخش ورودی نرون هستند که سیگنال‌ها را دریافت می‌کنند. ⚡️➡️🧠

---

**سؤال 3: تابع فعال‌سازی ReLU چگونه تعریف می‌شود؟**  
A) $f(x) = \tanh(x)$  
B) $f(x) = \frac{1}{1 + e^{-x}}$  
C) $f(x) = \max(0, x)$  
D) $f(x) = x^2$  
**پاسخ صحیح: C**  🔺⬅️📈

---

**سؤال 4: کدام نوع شبکه عصبی برای حل مشکل XOR معرفی شد؟**  
A) پرسپترون تک‌لایه  
B) شبکه بازخوردی  
C) شبکه چندلایه (MLP)  
D) RBF  
**پاسخ صحیح: C**  
**توضیح:** مشکل XOR توسط شبکه‌های چندلایه قابل حل است. ❌➡️✅✨

---

**سؤال 5: در مدل پرسپترون، خروجی چه زمانی 1 خواهد بود؟**  
A) وقتی مجموع ورودی‌ها منفی باشد  
B) وقتی وزن‌ها برابر صفر باشند  
C) وقتی net ≥ 0 باشد  
D) وقتی net < 0 باشد  
**پاسخ صحیح: C**  ➕➕ = ✅

---

**سؤال 6: کدام مورد جزو انواع شبکه‌های عصبی نیست؟**  
A) Hopfield  
B) Feedforward  
C) Kohonen  
D) Hadoop  
**پاسخ صحیح: D**  
**توضیح:** Hadoop یک چارچوب برای مدیریت داده است، نه شبکه عصبی. 🧠❌💾

---

**سؤال 7: در شبکه‌های عصبی، "Overfitting" به چه معناست؟**  
A) آموزش کم  
B) افزایش تعداد نرون‌ها  
C) یادگیری بیش از حد روی داده‌های آموزش  
D) عملکرد بالا در داده‌های جدید  
**پاسخ صحیح: C**  📚📚📚❌

---

**سؤال 8: چه بخشی از نرون وظیفه انتقال سیگنال به نرون‌های دیگر را دارد؟**  
A) دندریت  
B) آکسون  
C) سیناپس  
D) سوما  
**پاسخ صحیح: B**  
**توضیح:** آکسون سیگنال را از بدنه سلول به نرون‌های دیگر منتقل می‌کند. 🧠➡️📤

---

**سؤال 9: در شبکه‌های عصبی، چه عاملی بین نرون‌ها ارتباط برقرار می‌کند؟**  
A) لایه پنهان  
B) تابع فعال‌سازی  
C) سیناپس  
D) گرادیان نزولی  
**پاسخ صحیح: C**  
**توضیح:** سیناپس‌ها پل ارتباطی نرون‌ها هستند. 🔗🧠🔗

---

**سؤال 10: کدامیک از گزینه‌ها نوعی تابع فعال‌سازی نیست؟**  
A) ReLU  
B) Tanh  
C) Logistic  
D) Mean Square  
**پاسخ صحیح: D**  
**توضیح:** میانگین مربعات خطا یک تابع خطاست، نه تابع فعال‌سازی. ⚠️➖❌

--- 


**سؤال 11: اولین مدل ریاضی نرون مصنوعی توسط چه کسانی ارائه شد؟**  
A) جان هاپفیلد  
B) کالمن و مک‌کورد  
C) پیتس و مک‌کلاخ  
D) سایمون و مینسکی  
**پاسخ صحیح: C**  
**توضیح:** مدل ریاضی نرون توسط پیتس و مک‌کلاخ در سال 1943 معرفی شد. 🧠✨

---

**سؤال 12: وظیفه اصلی دندریت‌ها در نرون چیست؟**  
A) ارسال سیگنال  
B) پردازش اطلاعات  
C) دریافت سیگنال  
D) تنظیم وزن‌ها  
**پاسخ صحیح: C**  
**توضیح:** دندریت‌ها بخش ورودی نرون هستند که سیگنال‌ها را دریافت می‌کنند. ⚡️➡️🧠

---

**سؤال 13: تابع فعال‌سازی ReLU چگونه تعریف می‌شود؟**  
A) $f(x) = \tanh(x)$  
B) $f(x) = \frac{1}{1 + e^{-x}}$  
C) $f(x) = \max(0, x)$  
D) $f(x) = x^2$  
**پاسخ صحیح: C**  🔺⬅️📈

---

**سؤال 14: کدام نوع شبکه عصبی برای حل مشکل XOR معرفی شد؟**  
A) پرسپترون تک‌لایه  
B) شبکه بازخوردی  
C) شبکه چندلایه (MLP)  
D) RBF  
**پاسخ صحیح: C**  
**توضیح:** مشکل XOR توسط شبکه‌های چندلایه قابل حل است. ❌➡️✅✨

---

**سؤال 15: در مدل پرسپترون، خروجی چه زمانی 1 خواهد بود؟**  
A) وقتی مجموع ورودی‌ها منفی باشد  
B) وقتی وزن‌ها برابر صفر باشند  
C) وقتی net ≥ 0 باشد  
D) وقتی net < 0 باشد  
**پاسخ صحیح: C**  ➕➕ = ✅

---

**سؤال 16: کدام مورد جزو انواع شبکه‌های عصبی نیست؟**  
A) Hopfield  
B) Feedforward  
C) Kohonen  
D) Hadoop  
**پاسخ صحیح: D**  
**توضیح:** Hadoop یک چارچوب برای مدیریت داده است، نه شبکه عصبی. 🧠❌💾

---

**سؤال 17: در شبکه‌های عصبی، "Overfitting" به چه معناست؟**  
A) آموزش کم  
B) افزایش تعداد نرون‌ها  
C) یادگیری بیش از حد روی داده‌های آموزش  
D) عملکرد بالا در داده‌های جدید  
**پاسخ صحیح: C**  📚📚📚❌

---

**سؤال 18: چه بخشی از نرون وظیفه انتقال سیگنال به نرون‌های دیگر را دارد؟**  
A) دندریت  
B) آکسون  
C) سیناپس  
D) سوما  
**پاسخ صحیح: B**  
**توضیح:** آکسون سیگنال را از بدنه سلول به نرون‌های دیگر منتقل می‌کند. 🧠➡️📤

---

**سؤال 19: در شبکه‌های عصبی، چه عاملی بین نرون‌ها ارتباط برقرار می‌کند؟**  
A) لایه پنهان  
B) تابع فعال‌سازی  
C) سیناپس  
D) گرادیان نزولی  
**پاسخ صحیح: C**  
**توضیح:** سیناپس‌ها پل ارتباطی نرون‌ها هستند. 🔗🧠🔗

---

**سؤال 20: کدامیک از گزینه‌ها نوعی تابع فعال‌سازی نیست؟**  
A) ReLU  
B) Tanh  
C) Logistic  
D) Mean Square  
**پاسخ صحیح: D**  
**توضیح:** میانگین مربعات خطا یک تابع خطاست، نه تابع فعال‌سازی. ⚠️➖❌

--- 
