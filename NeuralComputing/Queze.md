### سوالات و پاسخ‌های مربوط به نرون‌های مصنوعی

---

**سؤال 1:**
کدام گزینه نشان‌دهنده وظیفه اصلی سیناپس در نرون مصنوعی است؟  
A) دریافت ورودی  
B) تولید خروجی  
C) ذخیره‌سازی اطلاعات  
D) تعیین وزن اتصال بین نرون‌ها  
✅ **گزینه D** — سیناپس‌ها وزن اتصال بین نرون‌ها را مشخص می‌کنند و نقش حیاتی در یادگیری دارند ⚖️.

---

**سؤال 2:**
کدام تابع فعال‌سازی خروجی بین -1 تا 1 تولید می‌کند؟  
A) Sigmoid  
B) Tanh  
C) ReLU  
D) Linear  
✅ **گزینه B** — تابع **Tanh** خروجی بین **-1 تا 1** می‌دهد، در حالی‌که Sigmoid بین 0 تا 1 است 📈.

---

**سؤال 3:**
در الگوریتم گرادیان نزولی، تغییر وزن‌ها در چه جهتی صورت می‌گیرد؟  
A) در جهت افزایش تابع خطا  
B) در جهت کاهش تابع خطا  
C) به طور تصادفی  
D) همواره صفر  
✅ **گزینه B** — در گرادیان نزولی، حرکت در جهت **کاهش تابع خطا** انجام می‌شود 🔽.

---

**سؤال 4:**
شبکه پرسپترون تنها قادر به حل چه نوع مسائلی است؟  
A) مسائل غیرخطی  
B) مسائل چندکلاسه  
C) مسائل خطی جداپذیر  
D) مسائل بازگشتی  
✅ **گزینه C** — پرسپترون فقط می‌تواند **مسائل خطی جداپذیر** را حل کند (مثل AND)، نه XOR ❌.

---

**سؤال 5:**
در یادگیری بدون ناظر، شبکه چه کاری انجام می‌دهد؟  
A) از خروجی‌های مشخص برای تنظیم وزن‌ها استفاده می‌کند  
B) تنها ورودی‌ها را دریافت و خوشه‌بندی می‌کند  
C) از برچسب‌های داده برای پیش‌بینی استفاده می‌کند  
D) به تقویت یا تنبیه پاسخ‌ها متکی است  
✅ **گزینه B** — در یادگیری **بدون ناظر**، شبکه بر اساس شباهت ورودی‌ها عمل می‌کند، بدون برچسب 🧩.

---

**سؤال 6:**
کدامیک از گزینه‌ها جزو مزایای تابع ReLU نیست؟  
A) سادگی محاسبات  
B) افزایش سرعت یادگیری  
C) جلوگیری کامل از Overfitting  
D) جلوگیری از ناپدید شدن گرادیان  
✅ **گزینه C** — ReLU مزایای زیادی دارد، اما **جلوگیری کامل از Overfitting** وظیفه‌ی آن نیست 🚫.

---

**سؤال 7:**
در ساختار شبکه چندلایه (MLP)، لایه پنهان چه کاربردی دارد؟  
A) تبدیل سیگنال به خروجی نهایی  
B) فقط نگهداری اطلاعات  
C) مدل‌سازی روابط غیرخطی  
D) حذف داده‌های نویزی  
✅ **گزینه C** — لایه پنهان در شبکه MLP برای **مدل‌سازی روابط غیرخطی** بین داده‌ها استفاده می‌شود 🧠.

---

**سؤال 8:**
کدام شبکه در حل مسائل بهینه‌سازی و حافظه انجمنی کاربرد دارد؟  
A) Feedforward  
B) Kohonen  
C) Hopfield  
D) RBF  
✅ **گزینه C** — شبکه **Hopfield** برای حافظه انجمنی و مسائل بهینه‌سازی استفاده می‌شود 💾🔁.

---

**سؤال 9:**
در شبکه‌های عصبی، **Overfitting** زمانی رخ می‌دهد که:  
A) شبکه خطاهای ورودی را نادیده بگیرد  
B) شبکه روی داده‌های آموزشی بیش‌ازحد دقیق شود  
C) داده‌ها بسیار کم باشند  
D) تابع فعال‌سازی اشتباه انتخاب شده باشد  
✅ **گزینه B** — Overfitting یعنی شبکه بیش از حد روی داده‌های آموزش حساس شود و به داده‌های جدید خوب پاسخ ندهد ⚠️.

---

**سؤال 10:**
فرمول Kolmogorov برای تخمین نرون‌های لایه پنهان چیست؟  
A) n + 1  
B) m × n  
C) 2n + 1  
D) n² - 1  

✅ **گزینه C** — فرمول Kolmogorov برای تخمین نرون‌های لایه پنهان: **2n + 1** 📐.

---

### ✍️ **سؤال تشریحی 1**

**سؤال:**  
در یک پروژه‌ی طبقه‌بندی تصاویر (مثلاً تشخیص گربه و سگ)، از یک شبکه‌ی عصبی چندلایه استفاده شده است. توضیح دهید که هر کدام از موارد زیر چه نقشی در عملکرد شبکه دارد:

1. لایه پنهان
2. تابع فعال‌سازی ReLU
3. داده‌های اعتبارسنجی (Validation set)
4. الگوریتم گرادیان نزولی

📘 در پاسخ خود، سعی کنید نقش هر بخش را با ذکر مثال یا کاربرد توضیح دهید.

---

### ✅ **پاسخ سؤال تشریحی**

در پروژه‌ی طبقه‌بندی تصاویر (مثل تفکیک گربه و سگ 🐱🐶)، از شبکه‌های عصبی چندلایه استفاده می‌شود. هر جزء از این شبکه نقش مهمی در عملکرد نهایی دارد:

---

**1. لایه پنهان (Hidden Layer):**  
لایه‌های پنهان واسطه‌ای بین ورودی و خروجی هستند که **ویژگی‌های مهم تصویر** را استخراج می‌کنند 🧠🔍.  
به عنوان مثال، در تصویر گربه، ممکن است یک لایه پنهان لبه‌ی گوش را تشخیص دهد و لایه بعدی ترکیب آن با چشم را شناسایی کند. این باعث می‌شود شبکه روابط **غیرخطی و پیچیده** بین پیکسل‌ها را یاد بگیرد. به همین ترتیب، لایه‌های پنهان می‌توانند ویژگی‌های مختلفی مانند رنگ، شکل و بافت را شناسایی کنند.

---

**2. تابع فعال‌سازی ReLU:**  
تابع ReLU یا \( f(x) = \max(0, x) \) یکی از پرکاربردترین توابع در شبکه‌های عمیق است.  
این تابع باعث می‌شود **نرون‌ها فقط وقتی فعال شوند که سیگنال ورودی مثبت باشد** 🔛، که هم از نظر محاسباتی سریع است ⚡ و هم از بروز مشکل **vanishing gradient** جلوگیری می‌کند. به عنوان مثال، در شرایطی که ورودی منفی است، خروجی تابع ReLU صفر خواهد بود، که به نرون کمک می‌کند تا یادگیری بهتری داشته باشد و از محاسبات غیرضروری پرهیز کند.

---

**3. داده‌های اعتبارسنجی (Validation Set):**  
این داده‌ها در طول آموزش برای **ارزیابی مدل** استفاده می‌شوند تا مشخص شود آیا مدل فقط داده‌های آموزش را حفظ کرده (Overfitting) یا واقعاً یاد گرفته است 🧪.  
به عنوان مثال، اگر خطای آموزش پایین باشد اما در اعتبارسنجی بالا، این نشان‌دهنده‌ی Overfitting است و باید مدل اصلاح شود. به این ترتیب، داده‌های اعتبارسنجی به ما کمک می‌کنند تا کیفیت و عمومیت مدل را ارزیابی کنیم و در نتیجه تصمیمات بهتری برای بهبود مدل بگیریم.

---

**4. الگوریتم گرادیان نزولی (Gradient Descent):**  
این الگوریتم برای **به‌روزرسانی وزن‌ها** در جهت کاهش خطا استفاده می‌شود.  
با محاسبه‌ی شیب تابع خطا نسبت به وزن‌ها، مسیر حرکت برای کاهش خطا مشخص می‌شود ➘. به عنوان مثال، اگر مدل خروجی اشتباهی بدهد، گرادیان نزولی کمک می‌کند تا وزن‌ها طوری تنظیم شوند که در تکرارهای بعدی، خروجی بهبود یابد ✅. این فرآیند به تدریج وزن‌ها را به سمت مقادیر بهینه هدایت می‌کند و باعث بهبود عملکرد مدل می‌شود.

---

📝 **نتیجه‌گیری:**  
همه این اجزا به صورت هماهنگ با هم کار می‌کنند تا شبکه بتواند یک ورودی (مثل تصویر) را به درستی تحلیل کرده و نتیجه‌ای دقیق و قابل اعتماد ارائه دهد.

---

### ✍️ **سؤال تشریحی 2**

**سؤال:**  
الگوریتم گرادیان نزولی چه نقشی در فرآیند یادگیری شبکه عصبی ایفا می‌کند؟ نحوه عملکرد آن را توضیح دهید و بیان کنید چرا انتخاب نرخ یادگیری (Learning Rate) در این الگوریتم اهمیت دارد.

---

### ✅ **پاسخ نمونه: نقش الگوریتم گرادیان نزولی در یادگیری شبکه عصبی**

الگوریتم گرادیان نزولی در یادگیری شبکه عصبی، **نقش کلیدی در کاهش خطای مدل** دارد. 🎯 این الگوریتم در واقع تلاش می‌کند **تابع هزینه** (Cost Function) را **کمینه** کند. 📉

#### نحوه عملکرد الگوریتم گرادیان نزولی:

1. **محاسبه خطا:**  
   وقتی مدل خروجی‌ای تولید می‌کند، با مقایسه آن با خروجی واقعی، **خطا** محاسبه می‌شود. 🔍

2. **محاسبه گرادیان:**  
   سپس با محاسبه **مشتق (گرادیان)** این خطا نسبت به وزن‌ها، مشخص می‌شود که وزن‌ها باید در چه جهتی تغییر کنند. ➕

3. **به‌روزرسانی وزن‌ها:**  
   وزن‌ها در جهت **منفی گرادیان** تغییر داده می‌شوند تا در تکرارهای بعدی، خطای مدل کمتر شود. این فرآیند شبیه به **حرکت توپ از بالای کوه به سمت دره** است، که همواره در جهت شیب به پایین حرکت می‌کند تا به نقطه مینیمم برسد. 🏔️⬇️

#### اهمیت انتخاب نرخ یادگیری (Learning Rate):

نرخ یادگیری در این الگوریتم اهمیت زیادی دارد:

- **نرخ یادگیری خیلی کوچک:**  
  اگر نرخ یادگیری خیلی کوچک باشد، فرآیند یادگیری **کند** می‌شود و ممکن است در **مینیمم محلی** گیر کند. ⏳

- **نرخ یادگیری خیلی بزرگ:**  
  اگر نرخ یادگیری خیلی بزرگ باشد، ممکن است از نقطه مینیمم عبور کرده و شبکه هرگز به **مینیمم کلی (Global Minimum)** نرسد و حتی ناپایدار شود. ⚠️

---

با توجه به این توضیحات، می‌توان نتیجه گرفت که الگوریتم گرادیان نزولی و انتخاب مناسب نرخ یادگیری از عوامل حیاتی در بهینه‌سازی و عملکرد صحیح شبکه‌های عصبی هستند. 🤖✨
