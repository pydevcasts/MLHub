**سؤال 1:**
کدام گزینه نشان‌دهنده وظیفه اصلی سیناپس در نرون مصنوعی است؟
A) دریافت ورودی
B) تولید خروجی
C) ذخیره‌سازی اطلاعات
D) تعیین وزن اتصال بین نرون‌ها
✅

---

**سؤال 2:**
کدام تابع فعال‌سازی خروجی بین -1 تا 1 تولید می‌کند؟
A) Sigmoid
B) Tanh
C) ReLU
D) Linear
✅

---

**سؤال 3:**
در الگوریتم گرادیان نزولی، تغییر وزن‌ها در چه جهتی صورت می‌گیرد؟
A) در جهت افزایش تابع خطا
B) در جهت کاهش تابع خطا
C) به طور تصادفی
D) همواره صفر
✅

---

**سؤال 4:**
شبکه پرسپترون تنها قادر به حل چه نوع مسائلی است؟
A) مسائل غیرخطی
B) مسائل چندکلاسه
C) مسائل خطی جداپذیر
D) مسائل بازگشتی
✅

---

**سؤال 5:**
در یادگیری بدون ناظر، شبکه چه کاری انجام می‌دهد؟
A) از خروجی‌های مشخص برای تنظیم وزن‌ها استفاده می‌کند
B) تنها ورودی‌ها را دریافت و خوشه‌بندی می‌کند
C) از برچسب‌های داده برای پیش‌بینی استفاده می‌کند
D) به تقویت یا تنبیه پاسخ‌ها متکی است
✅

---

**سؤال 6:**
کدامیک از گزینه‌ها جزو مزایای تابع ReLU نیست؟
A) سادگی محاسبات
B) افزایش سرعت یادگیری
C) جلوگیری کامل از Overfitting
D) جلوگیری از ناپدید شدن گرادیان
✅

---

**سؤال 7:**
در ساختار شبکه چندلایه (MLP)، لایه پنهان چه کاربردی دارد؟
A) تبدیل سیگنال به خروجی نهایی
B) فقط نگهداری اطلاعات
C) مدل‌سازی روابط غیرخطی
D) حذف داده‌های نویزی
✅

---

**سؤال 8:**
کدام شبکه در حل مسائل بهینه‌سازی و حافظه انجمنی کاربرد دارد؟
A) Feedforward
B) Kohonen
C) Hopfield
D) RBF
✅

---

**سؤال 9:**
در شبکه‌های عصبی، **Overfitting** زمانی رخ می‌دهد که:
A) شبکه خطاهای ورودی را نادیده بگیرد
B) شبکه روی داده‌های آموزشی بیش‌ازحد دقیق شود
C) داده‌ها بسیار کم باشند
D) تابع فعال‌سازی اشتباه انتخاب شده باشد
✅

---

**سؤال 10:**
فرمول Kolmogorov برای تخمین نرون‌های لایه پنهان چیست؟
A) n + 1
B) m × n
C) 2n + 1
D) n² - 1
✅
---
### ✍️ **سؤال تشریحی **

**سؤال:**
در یک پروژه‌ی طبقه‌بندی تصاویر (مثلاً تشخیص گربه و سگ)، از یک شبکه‌ی عصبی چندلایه استفاده شده است.
توضیح دهید که هر کدام از موارد زیر چه نقشی در عملکرد شبکه دارد:

1. لایه پنهان
2. تابع فعال‌سازی ReLU
3. داده‌های اعتبارسنجی (Validation set)
4. الگوریتم گرادیان نزولی

📘 در پاسخ خود، سعی کنید نقش هر بخش را با ذکر مثال یا کاربرد توضیح دهید.

---
### **سؤال 

**الگوریتم گرادیان نزولی چه نقشی در فرآیند یادگیری شبکه عصبی ایفا می‌کند؟ 
نحوه عملکرد آن را توضیح دهید و بیان کنید چرا انتخاب نرخ یادگیری (Learning Rate) در این الگوریتم اهمیت دارد.**
---

**سؤال 1:**
✅ **گزینه D** — سیناپس‌ها وزن اتصال بین نرون‌ها رو مشخص می‌کنن و نقش حیاتی در یادگیری دارن ⚖️.

---

**سؤال 2:**
✅ **گزینه B** — تابع **Tanh** خروجی بین **-1 تا 1** می‌ده، در حالی‌که Sigmoid بین 0 تا 1 هست 📈.

---

**سؤال 3:**
✅ **گزینه B** — در گرادیان نزولی، حرکت در جهت **کاهش تابع خطا** انجام می‌شه 🔽.

---

**سؤال 4:**
✅ **گزینه C** — پرسپترون فقط می‌تونه **مسائل خطی جداپذیر** رو حل کنه (مثل AND)، نه XOR ❌.

---

**سؤال 5:**
✅ **گزینه B** — در یادگیری **بدون ناظر**، شبکه بر اساس شباهت ورودی‌ها عمل می‌کنه، بدون برچسب 🧩.

---

**سؤال 6:**
✅ **گزینه C** — ReLU مزایای زیادی داره، اما **جلوگیری کامل از Overfitting** وظیفه‌ی اون نیست 🚫.

---

**سؤال 7:**
✅ **گزینه C** — لایه پنهان در شبکه MLP برای **مدل‌سازی روابط غیرخطی** بین داده‌ها استفاده می‌شه 🧠.

---

**سؤال 8:**
✅ **گزینه C** — شبکه **Hopfield** برای حافظه انجمنی و مسائل بهینه‌سازی استفاده می‌شه 💾🔁.

---

**سؤال 9:**
✅ **گزینه B** — Overfitting یعنی شبکه بیش از حد روی داده‌های آموزش حساس بشه و به داده جدید خوب پاسخ نده ⚠️.

---

**سؤال 10:**
✅ **گزینه C** — فرمول Kolmogorov برای تخمین نرون‌های لایه پنهان: **2n + 1** 📐.

---
### ✅ **پاسخ  سؤال تشریحی**

در پروژه‌ی طبقه‌بندی تصاویر (مثل تفکیک گربه و سگ 🐱🐶)، از شبکه‌های عصبی چندلایه استفاده می‌شود. هر جزء از این شبکه نقش مهمی در عملکرد نهایی دارد:

---

**1. لایه پنهان (Hidden Layer):**
لایه‌های پنهان واسطه‌ای بین ورودی و خروجی هستند که **ویژگی‌های مهم تصویر** را استخراج می‌کنند 🧠🔍.
مثلاً در تصویر گربه، ممکن است یک لایه پنهان لبه‌ی گوش را تشخیص دهد و لایه بعدی ترکیب آن با چشم را تشخیص دهد. این باعث می‌شود شبکه روابط **غیرخطی و پیچیده** بین پیکسل‌ها را یاد بگیرد.

---

**2. تابع فعال‌سازی ReLU:**
تابع ReLU یا $f(x) = \max(0, x)$ یکی از پرکاربردترین توابع در شبکه‌های عمیق است.
این تابع باعث می‌شود **نرون‌ها فقط وقتی فعال شوند که سیگنال ورودی مثبت باشد** 🔛، که هم از نظر محاسباتی سریع است ⚡ و هم از بروز مشکل **vanishing gradient** جلوگیری می‌کند.

---

**3. داده‌های اعتبارسنجی (Validation Set):**
این داده‌ها در طول آموزش برای **ارزیابی مدل** استفاده می‌شوند تا مشخص شود آیا مدل فقط داده‌های آموزش را حفظ کرده (Overfitting) یا واقعاً یاد گرفته است 🧪.
مثلاً اگر خطای آموزش پایین باشد اما در اعتبارسنجی بالا، باید مدل اصلاح شود.

---

**4. الگوریتم گرادیان نزولی (Gradient Descent):**
این الگوریتم برای **به‌روزرسانی وزن‌ها** در جهت کاهش خطا استفاده می‌شود.
با محاسبه‌ی شیب تابع خطا نسبت به وزن‌ها، مسیر حرکت برای کاهش خطا مشخص می‌شود ➘.
به عنوان مثال، اگر مدل خروجی اشتباهی بدهد، گرادیان نزولی کمک می‌کند تا وزن‌ها طوری تنظیم شوند که در تکرارهای بعدی، خروجی بهبود یابد ✅.

---

📝 **نتیجه‌گیری:**
همه این اجزا به صورت هماهنگ با هم کار می‌کنند تا شبکه بتواند یک ورودی (مثل تصویر) را به درستی تحلیل کرده و نتیجه‌ای دقیق و قابل اعتماد ارائه دهد.

---
آفرین! پاسخ بسیار خوبی نوشتی. من فقط کمی ساختار متن و نگارش رو بهتر می‌کنم تا هم برای ارائه، هم برای نوشتن در امتحان مناسب‌تر و مرتب‌تر باشه:

---

### **پاسخ نمونه: نقش الگوریتم گرادیان نزولی در یادگیری شبکه عصبی**

الگوریتم گرادیان نزولی در یادگیری شبکه عصبی، **نقش کلیدی در کاهش خطای مدل** دارد. این الگوریتم در واقع تلاش می‌کند **تابع هزینه** (Cost Function) را **کمینه** کند.

وقتی مدل خروجی‌ای تولید می‌کند، با مقایسه آن با خروجی واقعی، **خطا** محاسبه می‌شود. سپس با محاسبه **مشتق (گرادیان)** این خطا نسبت به وزن‌ها، مشخص می‌شود که وزن‌ها باید در چه جهتی تغییر کنند.

وزن‌ها در جهت **منفی گرادیان** تغییر داده می‌شوند تا در تکرارهای بعدی، خطای مدل کمتر شود. این فرآیند شبیه به **حرکت توپ از بالای کوه به سمت دره** است، که همواره در جهت شیب به پایین حرکت می‌کند تا به نقطه مینیمم برسد.

**نرخ یادگیری (Learning Rate)** در این الگوریتم اهمیت زیادی دارد:

* اگر **خیلی کوچک** باشد، فرآیند یادگیری **کند** شده و ممکن است در **مینیمم محلی** گیر کند.
* اگر **خیلی بزرگ** باشد، ممکن است از نقطه مینیمم عبور کرده و شبکه هرگز به **مینیمم کلی (Global Minimum)** نرسد و حتی ناپایدار شود.

---

**ایموجی‌ها برای زیباسازی:**


