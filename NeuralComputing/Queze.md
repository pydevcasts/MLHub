### سوالات و پاسخ‌های مربوط به نرون‌های مصنوعی

---

**سؤال 1:**
کدام گزینه نشان‌دهنده وظیفه اصلی سیناپس در نرون مصنوعی است؟  
A) دریافت ورودی  
B) تولید خروجی  
C) ذخیره‌سازی اطلاعات  
D) تعیین وزن اتصال بین نرون‌ها  
✅ **گزینه D** — سیناپس‌ها وزن اتصال بین نرون‌ها را مشخص می‌کنند و نقش حیاتی در یادگیری دارند ⚖️.

---

**سؤال 2:**
کدام تابع فعال‌سازی خروجی بین -1 تا 1 تولید می‌کند؟  
A) Sigmoid  
B) Tanh  
C) ReLU  
D) Linear  
✅ **گزینه B** — تابع **Tanh** خروجی بین **-1 تا 1** می‌دهد، در حالی‌که Sigmoid بین 0 تا 1 است 📈.

---

**سؤال 3:**
در الگوریتم گرادیان نزولی، تغییر وزن‌ها در چه جهتی صورت می‌گیرد؟  
A) در جهت افزایش تابع خطا  
B) در جهت کاهش تابع خطا  
C) به طور تصادفی  
D) همواره صفر  
✅ **گزینه B** — در گرادیان نزولی، حرکت در جهت **کاهش تابع خطا** انجام می‌شود 🔽.

---

**سؤال 4:**
شبکه پرسپترون تنها قادر به حل چه نوع مسائلی است؟  
A) مسائل غیرخطی  
B) مسائل چندکلاسه  
C) مسائل خطی جداپذیر  
D) مسائل بازگشتی  
✅ **گزینه C** — پرسپترون فقط می‌تواند **مسائل خطی جداپذیر** را حل کند (مثل AND)، نه XOR ❌.

---

**سؤال 5:**
در یادگیری بدون ناظر، شبکه چه کاری انجام می‌دهد؟  
A) از خروجی‌های مشخص برای تنظیم وزن‌ها استفاده می‌کند  
B) تنها ورودی‌ها را دریافت و خوشه‌بندی می‌کند  
C) از برچسب‌های داده برای پیش‌بینی استفاده می‌کند  
D) به تقویت یا تنبیه پاسخ‌ها متکی است  
✅ **گزینه B** — در یادگیری **بدون ناظر**، شبکه بر اساس شباهت ورودی‌ها عمل می‌کند، بدون برچسب 🧩.

---

**سؤال 6:**
کدامیک از گزینه‌ها جزو مزایای تابع ReLU نیست؟  
A) سادگی محاسبات  
B) افزایش سرعت یادگیری  
C) جلوگیری کامل از Overfitting  
D) جلوگیری از ناپدید شدن گرادیان  
✅ **گزینه C** — ReLU مزایای زیادی دارد، اما **جلوگیری کامل از Overfitting** وظیفه‌ی آن نیست 🚫.

---

**سؤال 7:**
در ساختار شبکه چندلایه (MLP)، لایه پنهان چه کاربردی دارد؟  
A) تبدیل سیگنال به خروجی نهایی  
B) فقط نگهداری اطلاعات  
C) مدل‌سازی روابط غیرخطی  
D) حذف داده‌های نویزی  
✅ **گزینه C** — لایه پنهان در شبکه MLP برای **مدل‌سازی روابط غیرخطی** بین داده‌ها استفاده می‌شود 🧠.

---

**سؤال 8:**
کدام شبکه در حل مسائل بهینه‌سازی و حافظه انجمنی کاربرد دارد؟  
A) Feedforward  
B) Kohonen  
C) Hopfield  
D) RBF  
✅ **گزینه C** — شبکه **Hopfield** برای حافظه انجمنی و مسائل بهینه‌سازی استفاده می‌شود 💾🔁.

---

**سؤال 9:**
در شبکه‌های عصبی، **Overfitting** زمانی رخ می‌دهد که:  
A) شبکه خطاهای ورودی را نادیده بگیرد  
B) شبکه روی داده‌های آموزشی بیش‌ازحد دقیق شود  
C) داده‌ها بسیار کم باشند  
D) تابع فعال‌سازی اشتباه انتخاب شده باشد  
✅ **گزینه B** — Overfitting یعنی شبکه بیش از حد روی داده‌های آموزش حساس شود و به داده‌های جدید خوب پاسخ ندهد ⚠️.

---

**سؤال 10:**
فرمول Kolmogorov برای تخمین نرون‌های لایه پنهان چیست؟  
A) n + 1  
B) m × n  
C) 2n + 1  
D) n² - 1  
✅ **گزینه C** — فرمول Kolmogorov برای تخمین نرون‌های لایه پنهان: **2n + 1** 📐.
---
### ✅ **پاسخ سؤال تشریحی**

در پروژه‌ی طبقه‌بندی تصاویر (مثل تفکیک گربه و سگ 🐱🐶)، از شبکه‌های عصبی چندلایه استفاده می‌شود. هر جزء از این شبکه نقش مهمی در عملکرد نهایی دارد:

---

**1. لایه پنهان (Hidden Layer):**  
لایه‌های پنهان واسطه‌ای بین ورودی و خروجی هستند که **ویژگی‌های مهم تصویر** را استخراج می‌کنند 🧠🔍.  
به عنوان مثال، در تصویر گربه، ممکن است یک لایه پنهان لبه‌ی گوش را تشخیص دهد و لایه بعدی ترکیب آن با چشم را شناسایی کند. این لایه‌ها به شبکه کمک می‌کنند تا روابط **غیرخطی و پیچیده** بین پیکسل‌ها و ویژگی‌های تصویر را یاد بگیرد. بدون این لایه‌ها، شبکه فقط می‌تواند الگوهای خطی را شناسایی کند و توانایی تشخیص ویژگی‌های پیچیده مانند شکل، بافت و رنگ را نخواهد داشت.

---

**2. تابع فعال‌سازی ReLU:**  
تابع ReLU (Rectified Linear Unit) که به صورت $f(x) = \max(0, x)$ تعریف می‌شود، یکی از پرکاربردترین توابع در شبکه‌های عمیق است.  
این تابع باعث می‌شود **نرون‌ها فقط وقتی فعال شوند که سیگنال ورودی مثبت باشد** 🔛. این ویژگی به شبکه کمک می‌کند تا به سرعت یاد بگیرد و از بروز مشکل **vanishing gradient** جلوگیری کند. به عنوان مثال، در حین آموزش، اگر نرونی سیگنال منفی دریافت کند، به سادگی خاموش می‌شود و وزن‌های آن به‌روزرسانی نمی‌شوند. این امر باعث می‌شود که شبکه بتواند ویژگی‌های مهم را به طور مؤثرتری یاد بگیرد و عملکرد بهتری داشته باشد.

---

**3. داده‌های اعتبارسنجی (Validation Set):**  
این داده‌ها در طول آموزش برای **ارزیابی مدل** استفاده می‌شوند تا مشخص شود آیا مدل فقط داده‌های آموزش را حفظ کرده (Overfitting) یا واقعاً یاد گرفته است 🧪.  
به عنوان مثال، اگر خطای آموزش پایین باشد اما خطای اعتبارسنجی بالا باشد، این نشان‌دهنده‌ی Overfitting است و به ما این امکان را می‌دهد که مدل را اصلاح کنیم، مثلاً با استفاده از تکنیک‌هایی مانند Dropout یا تنظیمات بهینه‌سازی. داده‌های اعتبارسنجی به ما کمک می‌کنند تا مطمئن شویم که مدل می‌تواند به درستی بر روی داده‌های جدید عمل کند.

---

**4. الگوریتم گرادیان نزولی (Gradient Descent):**  
این الگوریتم برای **به‌روزرسانی وزن‌ها** در جهت کاهش خطا استفاده می‌شود.  
با محاسبه‌ی شیب تابع خطا نسبت به وزن‌ها، مسیر حرکت برای کاهش خطا مشخص می‌شود ➘. به عنوان مثال، اگر مدل خروجی اشتباهی بدهد، گرادیان نزولی کمک می‌کند تا وزن‌ها طوری تنظیم شوند که در تکرارهای بعدی، خروجی بهبود یابد ✅. این فرآیند به صورت تکراری انجام می‌شود تا زمانی که مدل به یک نقطه بهینه در فضای پارامترها برسد.

---

📝 **نتیجه‌گیری:**  
همه این اجزا به صورت هماهنگ با هم کار می‌کنند تا شبکه بتواند یک ورودی (مثل تصویر) را به درستی تحلیل کرده و نتیجه‌ای دقیق و قابل اعتماد ارائه دهد. به این ترتیب، با استفاده از لایه‌های پنهان، توابع فعال‌سازی، داده‌های اعتبارسنجی و الگوریتم‌های بهینه‌سازی، شبکه‌های عصبی می‌توانند به طور مؤثری در پروژه‌های طبقه‌بندی تصاویر عمل کنند.
---
