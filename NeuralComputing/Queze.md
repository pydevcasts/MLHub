با کمال میل! در ادامه تمام سوالات و پاسخ‌ها را **بدون هیچ حذفی**، فقط با استفاده از ایموجی‌ها برای زیباتر شدن و جذاب‌تر شدن ساختار بازنویسی کردم:

---

### 🧠 **سؤال 1:**

کدام گزینه نشان‌دهنده وظیفه اصلی **سیناپس** در نرون مصنوعی است؟
A) دریافت ورودی
B) تولید خروجی
C) ذخیره‌سازی اطلاعات
D) **تعیین وزن اتصال بین نرون‌ها** ✅

🟩 **پاسخ:** گزینه D — سیناپس‌ها وزن اتصال بین نرون‌ها رو مشخص می‌کنن و نقش حیاتی در یادگیری دارن ⚖️

---

### 🌀 **سؤال 2:**

کدام تابع فعال‌سازی خروجی بین -1 تا 1 تولید می‌کند؟
A) Sigmoid
B) **Tanh** ✅
C) ReLU
D) Linear

🟩 **پاسخ:** گزینه B — تابع **Tanh** خروجی بین **-1 تا 1** می‌ده، در حالی‌که Sigmoid بین 0 تا 1 هست 📈

---

### ⬇️ **سؤال 3:**

در الگوریتم گرادیان نزولی، تغییر وزن‌ها در چه جهتی صورت می‌گیرد؟
A) در جهت افزایش تابع خطا
B) **در جهت کاهش تابع خطا** ✅
C) به طور تصادفی
D) همواره صفر

🟩 **پاسخ:** گزینه B — در گرادیان نزولی، حرکت در جهت **کاهش تابع خطا** انجام می‌شه 🔽

---

### 🔳 **سؤال 4:**

شبکه پرسپترون تنها قادر به حل چه نوع مسائلی است؟
A) مسائل غیرخطی
B) مسائل چندکلاسه
C) **مسائل خطی جداپذیر** ✅
D) مسائل بازگشتی

🟩 **پاسخ:** گزینه C — پرسپترون فقط می‌تونه **مسائل خطی جداپذیر** رو حل کنه (مثل AND)، نه XOR ❌

---

### 🧩 **سؤال 5:**

در یادگیری بدون ناظر، شبکه چه کاری انجام می‌دهد؟
A) از خروجی‌های مشخص برای تنظیم وزن‌ها استفاده می‌کند
B) **تنها ورودی‌ها را دریافت و خوشه‌بندی می‌کند** ✅
C) از برچسب‌های داده برای پیش‌بینی استفاده می‌کند
D) به تقویت یا تنبیه پاسخ‌ها متکی است

🟩 **پاسخ:** گزینه B — در یادگیری **بدون ناظر**، شبکه بر اساس شباهت ورودی‌ها عمل می‌کنه، بدون برچسب 🧩

---

### ⚠️ **سؤال 6:**

کدامیک از گزینه‌ها جزو مزایای تابع ReLU نیست؟
A) سادگی محاسبات
B) افزایش سرعت یادگیری
C) **جلوگیری کامل از Overfitting** ✅
D) جلوگیری از ناپدید شدن گرادیان

🟩 **پاسخ:** گزینه C — ReLU مزایای زیادی داره، اما **جلوگیری کامل از Overfitting** وظیفه‌ی اون نیست 🚫

---

### 🧱 **سؤال 7:**

در ساختار شبکه چندلایه (MLP)، لایه پنهان چه کاربردی دارد؟
A) تبدیل سیگنال به خروجی نهایی
B) فقط نگهداری اطلاعات
C) **مدل‌سازی روابط غیرخطی** ✅
D) حذف داده‌های نویزی

🟩 **پاسخ:** گزینه C — لایه پنهان در شبکه MLP برای **مدل‌سازی روابط غیرخطی** بین داده‌ها استفاده می‌شه 🧠

---

### 🔁 **سؤال 8:**

کدام شبکه در حل مسائل بهینه‌سازی و حافظه انجمنی کاربرد دارد؟
A) Feedforward
B) Kohonen
C) **Hopfield** ✅
D) RBF

🟩 **پاسخ:** گزینه C — شبکه **Hopfield** برای حافظه انجمنی و مسائل بهینه‌سازی استفاده می‌شه 💾🔁

---

### 🧪 **سؤال 9:**

در شبکه‌های عصبی، **Overfitting** زمانی رخ می‌دهد که:
A) شبکه خطاهای ورودی را نادیده بگیرد
B) **شبکه روی داده‌های آموزشی بیش‌ازحد دقیق شود** ✅
C) داده‌ها بسیار کم باشند
D) تابع فعال‌سازی اشتباه انتخاب شده باشد

🟩 **پاسخ:** گزینه B — Overfitting یعنی شبکه بیش از حد روی داده‌های آموزش حساس بشه و به داده جدید خوب پاسخ نده ⚠️

---

### 📐 **سؤال 10:**

فرمول Kolmogorov برای تخمین نرون‌های لایه پنهان چیست؟
A) n + 1
B) m × n
C) **2n + 1** ✅
D) n² - 1

🟩 **پاسخ:** گزینه C — فرمول Kolmogorov برای تخمین نرون‌های لایه پنهان: **2n + 1** 📐

---

## ✍️ **سؤال تشریحی 1: نقش اجزای شبکه در پروژه‌ی طبقه‌بندی تصاویر 🖼**

در پروژه‌ی طبقه‌بندی تصاویر (مثل تشخیص گربه و سگ 🐱🐶)، از شبکه‌های عصبی چندلایه استفاده می‌شود. هر جزء از این شبکه نقش مهمی دارد:

---

### 🔹 1. لایه پنهان (Hidden Layer):

ویژگی‌های مهم تصویر را استخراج می‌کند؛ مثلاً لبه گوش یا چشم گربه را شناسایی می‌کند. این لایه‌ها روابط غیرخطی را مدل‌سازی می‌کنند 🔍.

---

### 🔹 2. تابع ReLU:

فقط وقتی ورودی مثبت باشد، نرون فعال می‌شود. سریع، ساده و مؤثر است و از ناپدید شدن گرادیان جلوگیری می‌کند ⚡.

---

### 🔹 3. داده‌های اعتبارسنجی (Validation Set):

برای ارزیابی عملکرد شبکه در طول آموزش به کار می‌رود؛ اگر فقط در آموزش خوب باشد ولی در اعتبارسنجی بد، یعنی Overfitting رخ داده 🧪.

---

### 🔹 4. الگوریتم گرادیان نزولی (Gradient Descent):

با کاهش خطا از طریق به‌روزرسانی وزن‌ها، به مدل کمک می‌کند خروجی بهتر بدهد. مثل توپ که از کوه پایین می‌افتد تا به دره برسد 🎯.

---

## 🧠 **سؤال تشریحی 2: الگوریتم گرادیان نزولی و نرخ یادگیری**

الگوریتم گرادیان نزولی در یادگیری شبکه عصبی نقش کلیدی دارد. این الگوریتم با محاسبه‌ی **گرادیان تابع خطا**، وزن‌ها را در جهت **کاهش خطا** به‌روزرسانی می‌کند ⬇️.

🏞 این فرآیند مثل حرکت توپ از بالای کوه است؛ به سمت پایین و مینیمم خطا حرکت می‌کنیم.

**نرخ یادگیری (η)** نقش حیاتی دارد:

* اگر **خیلی کوچک باشد**، یادگیری بسیار کند می‌شود 🐢
* اگر **خیلی بزرگ باشد**، مدل ناپایدار می‌شود یا از مینیمم عبور می‌کند 🚫

🔑 انتخاب درست نرخ یادگیری باعث سرعت و دقت در آموزش شبکه می‌شود.

---
