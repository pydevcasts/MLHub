### سوال 1:
با توجه به مفهوم Agent-Environment Interface در یادگیری تقویتی، یک سناریوی واقعی را مثال بزنید که در آن agent باید با یک محیط non-Markovian تعامل داشته باشد. چالش‌های موجود در طراحی یک الگوریتم یادگیری تقویتی موثر برای این سناریو چیست؟

### پاسخ:

در یادگیری تقویتی، رابط **Agent-Environment** قلب فرآیند یادگیری را تشکیل می‌دهد. Agent در هر گام زمانی عملی را انتخاب کرده و در محیط اعمال می‌کند، سپس محیط بر اساس این عمل و حالت فعلی خود، پاداش و حالت بعدی را به Agent بازمی‌گرداند. مفهوم مارکووی بودن محیط بیانگر این است که حالت بعدی و پاداش تنها به حالت فعلی و عمل Agent بستگی دارد و از تاریخچه گذشته مستقل است. در مقابل، محیط‌های **non-Markovian** محیط‌هایی هستند که در آن‌ها حالت بعدی یا پاداش یا هر دو به تاریخچه کامل تعاملات گذشته (توالی حالت‌ها و عمل‌ها) بستگی دارد و تنها دانستن حالت فعلی برای پیش‌بینی آینده کافی نیست. این ویژگی عدم مارکووی بودن، چالش‌های قابل توجهی را در طراحی الگوریتم‌های یادگیری تقویتی ایجاد می‌کند.

#### سناریوی واقعی با محیط non-Markovian:

یک سناریوی واقعی که Agent در آن با یک محیط **non-Markovian** روبرو می‌شود، **ربات خودران** در یک محیط شهری پیچیده و پر ترافیک است. 🚗🏙️ در این سناریو:

- **Agent:** ربات خودران است که باید تصمیم بگیرد چگونه رانندگی کند (مانند تغییر مسیر، شتاب گرفتن، و ترمز کردن). 🛣️
- **Environment:** محیط شهری شامل سایر خودروها 🚙, عابران پیاده 🚶‍♂️, علائم راهنمایی و رانندگی 🛑, شرایط آب و هوایی ☔, و زیرساخت‌های جاده‌ای است.

**Non-Markovian Nature:** رفتار سایر خودروها و عابران پیاده در یک محیط شهری اغلب به سادگی قابل پیش‌بینی نیست و به تاریخچه تعاملات قبلی Agent و سایر عوامل محیطی بستگی دارد. برای مثال، یک راننده ممکن است به دلیل اینکه ربات خودران در گذشته سرعت خود را به طور ناگهانی کاهش داده است، به‌طور غیرمنتظره‌ای واکنش نشان دهد. همچنین، رفتار عابران پیاده ممکن است به این دلیل که آن‌ها مشاهده کرده‌اند ربات خودران به طور مداوم به آن‌ها راه می‌دهد، تغییر کند. حتی الگوهای ترافیکی کلی در یک شهر ممکن است بر اساس اتفاقات قبلی (مانند تصادفات ⚠️ یا رویدادهای خاص) تحت تأثیر قرار گیرند. بنابراین، تنها دانستن موقعیت فعلی ربات و سایر خودروها برای پیش‌بینی دقیق وضعیت بعدی کافی نیست؛ بلکه ربات نیاز به درک الگوهای رفتاری مبتنی بر تاریخچه تعاملات دارد.

#### چالش‌های موجود در طراحی یک الگوریتم یادگیری تقویتی موثر برای این سناریو:

1. **عدم قطعیت در مدل‌سازی محیط:** 🧩 مدل‌سازی یک محیط غیرمارکووی به‌طور دقیق بسیار دشوار است. از آنجایی که حالت بعدی و پاداش به تاریخچه بستگی دارد، تعریف دقیق فضای حالت که شامل اطلاعات کافی از تاریخچه باشد، می‌تواند بسیار پیچیده و با ابعاد بالا باشد. ساخت یک مدل دینامیکی کامل که بتواند تأثیر تاریخچه را در نظر بگیرد، تقریباً غیرممکن است. این عدم قطعیت در مدل، باعث می‌شود روش‌های مبتنی بر مدل (Model-based RL) که نیاز به یک مدل دقیق از محیط دارند، به خوبی کار نکنند.

2. **نیاز به حافظه برای ذخیره تاریخچه تعاملات:** 🧠 برای اینکه Agent بتواند با یک محیط non-Markovian به‌طور موثر تعامل کند، باید بتواند بخشی یا تمام تاریخچه تعاملات گذشته را به خاطر بسپارد. این تاریخچه می‌تواند شامل توالی حالت‌ها، عمل‌ها و پاداش‌های قبلی باشد. ذخیره و مدیریت این تاریخچه می‌تواند به سرعت منجر به افزایش فضای حافظه مورد نیاز شود. علاوه بر این، Agent باید یاد بگیرد که کدام بخش از تاریخچه برای تصمیم‌گیری‌های آینده مرتبط است، که خود یک چالش یادگیری اضافی است. استفاده از **شبکه‌های عصبی بازگشتی** (Recurrent Neural Networks - RNNs) یا **LSTM** برای حفظ یک حالت پنهان که خلاصه تاریخچه را در خود جای می‌دهد، می‌تواند مفید باشد، اما آموزش این مدل‌ها نیز پیچیدگی‌های خود را دارد.

3. **پیچیدگی در یافتن سیاست بهینه به دلیل فضای حالت بزرگ (شامل تاریخچه):** 🌌 اگر فضای حالت به گونه‌ای تعریف شود که شامل تاریخچه باشد، ابعاد آن به‌طور تصاعدی افزایش می‌یابد. این فضای حالت بزرگ، مسئله را به یک مسئله با فضای حالت بی‌نهایت یا بسیار بزرگ تبدیل می‌کند. در چنین فضایی، روش‌های سنتی یادگیری تقویتی که مقادیر را برای هر حالت به‌طور جداگانه تخمین می‌زنند (مانند جدول Q-learning)، غیرقابل استفاده می‌شوند. حتی با استفاده از **تقریب تابع** (Function Approximation)، یافتن یک سیاست بهینه در این فضای بزرگ و پیچیده، با چالش‌های همگرایی و نیاز به حجم عظیمی از داده‌های آموزشی روبرو است. مسئله هدف (Objective) نیز در این حالت پیچیده‌تر می‌شود، زیرا باید تابعی از تاریخچه باشد، نه تنها از حالت فعلی.





### سوال 2

با توجه به اهمیت تعادل بین Exploration و Exploitation در یادگیری تقویتی، به مقایسه دو الگوریتم UCB (Upper Confidence Bound) و ε-greedy بپردازید.  🌟

1. **تعریف کنید** که مفهوم Exploration و Exploitation چیست و چرا تعادل بین این دو در یادگیری تقویتی مهم است.
2. **توضیح دهید** که الگوریتم UCB چگونه بین Exploration و Exploitation تعادل برقرار می‌کند.
3. **مزایا و معایب** استفاده از UCB را در مقایسه با روش ε-greedy بیان کنید.
4. **در چه شرایطی** بهتر است از روش ε-greedy به‌جای UCB استفاده کنیم؟

### پاسخ نمونه

1. **مفهوم Exploration و Exploitation:** Exploration به معنای جستجو و آزمایش گزینه‌های جدید برای کسب اطلاعات بیشتر است، در حالی که Exploitation به معنای استفاده از اطلاعات موجود و انتخاب گزینه‌ای است که بهترین عملکرد را داشته است. تعادل بین این دو مهم است زیرا اگر فقط از یکی استفاده کنیم، ممکن است فرصت‌های خوبی را از دست بدهیم (در صورت عدم Exploration) یا نتایج ضعیفی کسب کنیم (در صورت عدم Exploitation).

2. **چگونگی تعادل در UCB:** الگوریتم UCB به‌جای استفاده از یک احتمال ثابت برای Exploration، از مفهوم "عدم اطمینان" استفاده می‌کند. برای هر گزینه، میانگین جایزه و عدم اطمینان آن را محاسبه می‌کند. UCB اسلاتی را انتخاب می‌کند که یا میانگین جایزه بالایی دارد یا عدم اطمینان زیادی در مورد آن وجود دارد.

3. **مزایا و معایب UCB نسبت به ε-greedy:**
   - **مزایا:** 
     - تعادل بهینه بین Exploration و Exploitation
     - همگرایی سریع‌تر به گزینه‌های بهینه
     - کارایی بالاتر در محیط‌های پیچیده
   - **معایب:**
     - پیچیدگی محاسباتی بیشتر
     - نیاز به تنظیم پارامترها
     - حساسیت به تخمین‌های اولیه

4. **شرایط استفاده از ε-greedy:** اگر سادگی و سهولت در پیاده‌سازی مهم‌تر باشد یا در شرایطی که تخمین‌های اولیه قابل اعتماد هستند، یا تعداد گزینه‌ها کم و تغییرات عملکرد گزینه‌ها زیاد نیست، روش ε-greedy گزینه مناسبی است.
