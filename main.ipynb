{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc63ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/pydevcasts/miniconda3/envs/gan/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/pydevcasts/miniconda3/envs/gan/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ceeed07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.12/site-packages (from datasets) (2.0.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.12/site-packages (from datasets) (4.67.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./venv/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./venv/lib/python3.12/site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Using cached propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m484.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m978.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.5/223.5 kB\u001b[0m \u001b[31m926.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (245 kB)\n",
      "Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.2/349.2 kB\u001b[0m \u001b[31m989.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 attrs-25.3.0 datasets-3.5.0 dill-0.3.8 frozenlist-1.6.0 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 pyarrow-19.0.1 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d92b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed, RepeatVector, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# دانلود و نصب nltk punkt برای تقسیم بندی جملات\n",
    "nltk.download('punkt')\n",
    "\n",
    "# بارگذاری مجموعه داده CNN/Daily Mail\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# انتخاب زیرمجموعه آموزش و تست\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "# استخراج مقالات و خلاصه‌ها\n",
    "X_train = [item['article'] for item in train_data]\n",
    "y_train = [item['highlights'] for item in train_data]\n",
    "X_test = [item['article'] for item in test_data]\n",
    "y_test = [item['highlights'] for item in test_data]\n",
    "\n",
    "# توکن‌سازی\n",
    "tokenizer_article = Tokenizer()\n",
    "tokenizer_article.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer_article.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer_article.texts_to_sequences(X_test)\n",
    "\n",
    "tokenizer_summary = Tokenizer()\n",
    "tokenizer_summary.fit_on_texts(y_train)\n",
    "y_train_seq = tokenizer_summary.texts_to_sequences(y_train)\n",
    "y_test_seq = tokenizer_summary.texts_to_sequences(y_test)\n",
    "\n",
    "# تنظیم حداکثر طول جملات\n",
    "max_len_article = max(len(seq) for seq in X_train_seq)\n",
    "max_len_summary = max(len(seq) for seq in y_train_seq)\n",
    "\n",
    "# پدینگ توالی‌ها\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len_article, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len_article, padding='post')\n",
    "y_train_pad = pad_sequences(y_train_seq, maxlen=max_len_summary, padding='post')\n",
    "y_test_pad = pad_sequences(y_test_seq, maxlen=max_len_summary, padding='post')\n",
    "\n",
    "# ساخت مدل\n",
    "vocab_size_article = len(tokenizer_article.word_index) + 1\n",
    "vocab_size_summary = len(tokenizer_summary.word_index) + 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len_article,)))\n",
    "model.add(Embedding(input_dim=vocab_size_article, output_dim=100))\n",
    "model.add(LSTM(100))\n",
    "model.add(RepeatVector(max_len_summary))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size_summary, activation='softmax')))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# آموزش مدل\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(X_train_pad, np.expand_dims(y_train_pad, -1), epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# ترسیم نمودار دقت و از دست دادن\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# دقت\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='دقت آموزش')\n",
    "plt.plot(history.history['val_accuracy'], label='دقت اعتبارسنجی')\n",
    "plt.title('دقت مدل')\n",
    "plt.xlabel('دوره')\n",
    "plt.ylabel('دقت')\n",
    "plt.legend()\n",
    "\n",
    "# از دست دادن\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='از دست دادن آموزش')\n",
    "plt.plot(history.history['val_loss'], label='از دست دادن اعتبارسنجی')\n",
    "plt.title('از دست دادن مدل')\n",
    "plt.xlabel('دوره')\n",
    "plt.ylabel('از دست دادن')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# تولید خلاصه\n",
    "def generate_summary(article):\n",
    "    seq = tokenizer_article.texts_to_sequences([article])\n",
    "    padded = pad_sequences(seq, maxlen=max_len_article, padding='post')\n",
    "    prediction = model.predict(padded)\n",
    "    predicted_summary = np.argmax(prediction, axis=-1)\n",
    "    return ' '.join(tokenizer_summary.index_word[i] for i in predicted_summary[0] if i != 0)\n",
    "\n",
    "# تست تولید خلاصه\n",
    "for article in X_test[:5]:  # فقط برای 5 مقاله تست\n",
    "    print(\"Article:\", article)\n",
    "    print(\"Generated Summary:\", generate_summary(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Input\n",
    "\n",
    "# فرض بر این است که داده‌ها و پیش‌پردازش مشابه کد قبلی هستند\n",
    "\n",
    "# تعریف تابع برای ایجاد مدل\n",
    "def create_model(embedding_dim, lstm_units):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(max_len_article,)))\n",
    "    model.add(Embedding(input_dim=vocab_size_article, output_dim=embedding_dim))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dense(vocab_size_summary, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# تابع برای ارزیابی عملکرد مدل\n",
    "def evaluate_model(individual):\n",
    "    embedding_dim, lstm_units = individual\n",
    "    model = create_model(embedding_dim, lstm_units)\n",
    "    model.fit(X_train_pad, np.expand_dims(y_train_pad, -1), epochs=1, batch_size=64, verbose=0)\n",
    "    predictions = model.predict(X_test_pad)\n",
    "    predicted_classes = np.argmax(predictions, axis=-1)\n",
    "    accuracy = accuracy_score(y_test_pad, predicted_classes)\n",
    "    return accuracy,\n",
    "\n",
    "# تنظیم DEAP\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"embedding_dim\", np.random.randint, 50, 200)  # Range for embedding dimension\n",
    "toolbox.register(\"lstm_units\", np.random.randint, 50, 200)  # Range for LSTM units\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual, (toolbox.embedding_dim, toolbox.lstm_units), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate_model)\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# اجرای الگوریتم ژنتیک\n",
    "population = toolbox.population(n=10)\n",
    "ngen = 5\n",
    "\n",
    "# ذخیره دقت هر نسل\n",
    "accuracy_per_generation = []\n",
    "\n",
    "for gen in range(ngen):\n",
    "    # انتخاب\n",
    "    offspring = toolbox.select(population, len(population))\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "    # ترکیب و جهش\n",
    "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "        if np.random.rand() < 0.5:\n",
    "            toolbox.mate(child1, child2)\n",
    "            del child1.fitness.values\n",
    "            del child2.fitness.values\n",
    "\n",
    "    for mutant in offspring:\n",
    "        if np.random.rand() < 0.2:\n",
    "            toolbox.mutate(mutant)\n",
    "            del mutant.fitness.values\n",
    "\n",
    "    # ارزیابی افراد جدید\n",
    "    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "    fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    # جمع آوری دقت هر نسل\n",
    "    fits = [ind.fitness.values[0] for ind in population]\n",
    "    accuracy_per_generation.append(np.mean(fits))\n",
    "\n",
    "    # جایگزینی جمعیت\n",
    "    population[:] = offspring\n",
    "\n",
    "# بهترین فرد\n",
    "fits = [ind.fitness.values[0] for ind in population]\n",
    "best_ind = population[np.argmax(fits)]\n",
    "print(\"Best individual is: \", best_ind)\n",
    "\n",
    "# ترسیم دقت هر نسل\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, ngen + 1), accuracy_per_generation, marker='o')\n",
    "plt.title('Average Accuracy per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.grid()\n",
    "plt.xticks(range(1, ngen + 1))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
